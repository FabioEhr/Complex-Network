{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnetworkx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnx\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_stationary_distribution\u001b[39m(P: np.ndarray,\n\u001b[32m     10\u001b[39m                              max_iter: \u001b[38;5;28mint\u001b[39m = \u001b[32m10_000\u001b[39m,\n\u001b[32m     11\u001b[39m                              tol: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m1e-12\u001b[39m) -> np.ndarray:\n\u001b[32m     12\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Power‑iteration to get the left stationary distribution π of P.\"\"\"\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from typing import Dict, Any, Hashable\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def _stationary_distribution(P: np.ndarray,\n",
    "                             max_iter: int = 10_000,\n",
    "                             tol: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"Power‑iteration to get the left stationary distribution π of P.\"\"\"\n",
    "    n = P.shape[0]\n",
    "    π = np.full(n, 1.0 / n)\n",
    "    for _ in range(max_iter):\n",
    "        π_new = π @ P\n",
    "        if np.linalg.norm(π_new - π, 1) < tol:\n",
    "            return π_new\n",
    "        π = π_new\n",
    "    return π  # warn: may not have converged\n",
    "\n",
    "\n",
    "def directed_random_walk_betweenness(\n",
    "    G: nx.DiGraph,\n",
    "    assume_strongly_connected: bool = False,\n",
    "    verbose: bool = True,\n",
    ") -> Dict[Hashable, float]:\n",
    "    \"\"\"Exact random‑walk betweenness for a **directed** graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : nx.DiGraph\n",
    "        Weighted or unweighted, **strongly‑connected** digraph.\n",
    "        If dangling nodes exist (out‑degree 0) they are given a uniform\n",
    "        teleport to keep the chain irreducible.\n",
    "    assume_strongly_connected : bool, default False\n",
    "        If *True* skip the connectivity test (saves a few seconds\n",
    "        on large graphs but may raise if the graph is really not SC).\n",
    "    verbose : bool, default True\n",
    "        Print progress and timings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping node → random‑walk betweenness centrality\n",
    "        (unnormalized).\n",
    "    \"\"\"\n",
    "\n",
    "    t_tot = time.perf_counter()\n",
    "    n = G.number_of_nodes()\n",
    "    nodes = list(G.nodes())\n",
    "    index = {u: i for i, u in enumerate(nodes)}\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1) Transition matrix P (row‑stochastic)\n",
    "    # ------------------------------------------------------------------\n",
    "    if verbose:\n",
    "        print(f\"[1/5] Building transition matrix  P  ({n}×{n}) …\", flush=True)\n",
    "    P = np.zeros((n, n), dtype=float)\n",
    "\n",
    "    for u in G:\n",
    "        i = index[u]\n",
    "        succ = list(G.successors(u))\n",
    "        if succ:\n",
    "            w_total = sum(G[u][v].get(\"weight\", 1.0) for v in succ)\n",
    "            for v in succ:\n",
    "                j = index[v]\n",
    "                P[i, j] = G[u][v].get(\"weight\", 1.0) / w_total\n",
    "        else:\n",
    "            # dangling → uniform teleport\n",
    "            P[i, :] = 1.0 / n\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2) Stationary distribution π  (left eigenvector of P)\n",
    "    # ------------------------------------------------------------------\n",
    "    if verbose:\n",
    "        print(\"[2/5] Power‑iteration for stationary distribution π …\",\n",
    "              flush=True)\n",
    "    t0 = time.perf_counter()\n",
    "    π = _stationary_distribution(P)\n",
    "    if verbose:\n",
    "        print(f\"      converged in {time.perf_counter() - t0:6.2f}s\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3) Random‑walk Laplacian  L_rw  and its pseudoinverse  M\n",
    "    # ------------------------------------------------------------------\n",
    "    if verbose:\n",
    "        print(\"[3/5] Forming Laplacian  L_rw  and dense pseudoinverse …\",\n",
    "              flush=True)\n",
    "    t0 = time.perf_counter()\n",
    "    I = np.eye(n, dtype=float)\n",
    "    L_rw = np.diag(π) @ (I - P)            # Π · (I − P)\n",
    "    M = np.linalg.pinv(L_rw, rcond=1e-12)  # Moore–Penrose pseudoinverse\n",
    "    if verbose:\n",
    "        print(f\"      SVD done in {time.perf_counter() - t0:6.2f}s\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4)   O(n²) accumulation   (vectorised inner loop)\n",
    "    # ------------------------------------------------------------------\n",
    "    if verbose:\n",
    "        print(\"[4/5] Accumulating betweenness (vectorised)…\", flush=True)\n",
    "    t0 = time.perf_counter()\n",
    "    diag_M = np.diag(M)\n",
    "    col_sum = M.sum(axis=0)\n",
    "    bet = np.zeros(n, dtype=float)\n",
    "\n",
    "    for j in tqdm(range(n), unit=\"node\"):\n",
    "        m_jj = diag_M[j]\n",
    "        csum_j = col_sum[j]\n",
    "\n",
    "        m_kj = M[:, j]         # column j\n",
    "        m_jk = M[j, :]         # row j\n",
    "        m_kk = diag_M          # broadcast\n",
    "\n",
    "        denom = m_jj - m_kj - m_jk + m_kk\n",
    "        # avoid /0 when k=j (value unused anyway)\n",
    "        denom[j] = np.inf\n",
    "\n",
    "        S = (csum_j - col_sum           # Σ_i m_ij − Σ_i m_ik\n",
    "             - m_jj + m_jk              #        - m_jj + m_jk\n",
    "             + (-n + 1) * m_kj          # (-n+1)·m_kj\n",
    "             + (n - 1) * m_kk)          # (n−1)·m_kk\n",
    "        S[j] = 0.0  # exclude k=j\n",
    "\n",
    "        bet[j] = np.divide(S, denom,\n",
    "                           out=np.zeros_like(S), where=denom != 0).sum()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"      done in {time.perf_counter() - t0:6.2f}s\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5) Wrap‑up\n",
    "    # ------------------------------------------------------------------\n",
    "    centrality = {nodes[j]: float(bet[j]) for j in range(n)}\n",
    "    if verbose:\n",
    "        print(f\"[5/5] All done in {time.perf_counter() - t_tot:6.2f}s\")\n",
    "\n",
    "    return centrality\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  Stand‑alone execution\n",
    "# ----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse, sys, pandas as pd\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Exact random‑walk betweenness for a directed graph\")\n",
    "    parser.add_argument(\"--parquet\", required=True,\n",
    "                        help=\"square adjacency matrix (parquet format)\")\n",
    "    parser.add_argument(\"--output\", default=\"rw_betweenness.npy\",\n",
    "                        help=\"output filename (.npy with dict of results)\")\n",
    "    parser.add_argument(\"--no‑verbose\", action=\"store_true\",\n",
    "                        help=\"suppress progress messages\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    verbose = not args.no_verbose\n",
    "    if verbose:\n",
    "        print(\"Loading sparsified adjacency matrix …\", file=sys.stderr)\n",
    "    import pandas as pd\n",
    "    df = pd.read_parquet(args.parquet)\n",
    "    df.index = df.columns\n",
    "    G = nx.from_pandas_adjacency(df, create_using=nx.DiGraph())\n",
    "\n",
    "    res = directed_random_walk_betweenness(G, verbose=verbose)\n",
    "    np.save(args.output, res)\n",
    "    if verbose:\n",
    "        print(f\"Saved betweenness dict to  {args.output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote edges.csv (shape: (396891, 3) )\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) load the Parquet\n",
    "df = pd.read_parquet(\"dfz_s_2010.parquet\")\n",
    "\n",
    "# 2) melt to edge list of positive weights\n",
    "rows, cols = df.shape\n",
    "# df is a square DataFrame: rows=cols=2063\n",
    "idx = (df > 0).stack()\n",
    "edges = idx[idx].reset_index()      # only the True entries\n",
    "edges.columns = [\"i\", \"j\", \"keep\"]\n",
    "edges[\"w\"] = df.values[ df.values > 0 ]  # grab the positive values\n",
    "edges = edges.loc[:, [\"i\",\"j\",\"w\"]]\n",
    "\n",
    "# 3) save\n",
    "edges.to_csv(\"edges.csv\", index=False)\n",
    "print(\"Wrote edges.csv (shape:\", edges.shape, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sparsified adjacency matrix from dfz_s_2010.parquet ...\n",
      "Loaded DataFrame of shape (2063, 2063)\n",
      "Directed graph has 396891 edges and 2063 nodes.\n",
      "\n",
      "Estimating the total betweenness centrality computation time ...\n",
      "Estimating runtime based on 10 sample nodes:\n",
      "  Node AUS_AGRagr: 0.0705 seconds\n",
      "  Node AUS_AGRfor: 0.0663 seconds\n",
      "  Node AUS_AGRfis: 0.0657 seconds\n",
      "  Node AUS_ENRcoa: 0.0665 seconds\n",
      "  Node AUS_ENRoil: 0.0667 seconds\n",
      "  Node AUS_ENRgas: 0.0652 seconds\n",
      "  Node AUS_MIN+: 0.0645 seconds\n",
      "  Node AUS_MANfoo: 0.0636 seconds\n",
      "  Node AUS_MANtex: 0.0649 seconds\n",
      "  Node AUS_MANwoo: 0.0643 seconds\n",
      "Average time per node: 0.0658 seconds\n",
      "Estimated total computation time: 135.75 seconds (~ 2.26 minutes)\n",
      "\n",
      "Starting actual betweenness centrality calculation ...\n",
      "Actual betweenness centrality computed in 176.55 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import time\n",
    "\n",
    "def estimate_betweenness_runtime(G, sample_size=10, weight='weight'):\n",
    "    \"\"\"\n",
    "    Estimate the time required to compute betweenness centrality on a graph G.\n",
    "    \n",
    "    The function samples `sample_size` nodes and computes the time needed to\n",
    "    run a single-source shortest path calculation (as used internally by betweenness centrality)\n",
    "    and then multiplies the average time by the total number of nodes.\n",
    "    \n",
    "    Returns the estimated total time in seconds.\n",
    "    \"\"\"\n",
    "    nodes = list(G.nodes())\n",
    "    if sample_size > len(nodes):\n",
    "        sample_size = len(nodes)\n",
    "    sample_nodes = nodes[:sample_size]\n",
    "    times = []\n",
    "    \n",
    "    print(f\"Estimating runtime based on {sample_size} sample nodes:\")\n",
    "    for node in sample_nodes:\n",
    "        t0 = time.time()\n",
    "        # Use the weighted single_source_dijkstra_path_length,\n",
    "        # which is similar to the work done per node in betweenness centrality.\n",
    "        _ = nx.single_source_dijkstra_path_length(G, node, weight=weight)\n",
    "        t1 = time.time()\n",
    "        elapsed = t1 - t0\n",
    "        times.append(elapsed)\n",
    "        print(f\"  Node {node}: {elapsed:.4f} seconds\")\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    estimated_total_time = avg_time * len(nodes)\n",
    "    \n",
    "    print(f\"Average time per node: {avg_time:.4f} seconds\")\n",
    "    return estimated_total_time\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Read the sparsified graph\n",
    "# -------------------------------\n",
    "print(\"Loading sparsified adjacency matrix from dfz_s_2010.parquet ...\")\n",
    "df = pd.read_parquet('dfz_s_2010.parquet')\n",
    "# Assuming the DataFrame is a square adjacency matrix so that rows match columns\n",
    "df.index = df.columns  \n",
    "print(f\"Loaded DataFrame of shape {df.shape}\")\n",
    "\n",
    "# Create a directed graph from the DataFrame:\n",
    "G_directed = nx.from_pandas_adjacency(df, create_using=nx.DiGraph())\n",
    "print(f\"Directed graph has {G_directed.number_of_edges()} edges and {G_directed.number_of_nodes()} nodes.\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Estimate the total runtime for betweenness centrality\n",
    "# -------------------------------\n",
    "print(\"Estimating the total betweenness centrality computation time ...\")\n",
    "estimated_time_sec = estimate_betweenness_runtime(G_directed, sample_size=10, weight='weight')\n",
    "print(f\"Estimated total computation time: {estimated_time_sec:.2f} seconds (~ {estimated_time_sec/60:.2f} minutes)\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Compute Normal Betweenness Centrality (Directed)\n",
    "# -------------------------------\n",
    "print(\"Starting actual betweenness centrality calculation ...\")\n",
    "start_time = time.time()\n",
    "\n",
    "bc_directed = nx.betweenness_centrality(G_directed,\n",
    "                                        normalized=True,\n",
    "                                        weight='weight',\n",
    "                                        endpoints=False)\n",
    "end_time = time.time()\n",
    "actual_time_sec = end_time - start_time\n",
    "print(f\"Actual betweenness centrality computed in {actual_time_sec:.2f} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated triple-quoted string literal (detected at line 45) (3622392847.py, line 39)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated triple-quoted string literal (detected at line 45)\n"
     ]
    }
   ],
   "source": [
    "r_script = \"\"\"\\\n",
    "# compute_rwbc.R\n",
    "# --------------\n",
    "\n",
    "```python\n",
    "# This Python code will generate an R script for computing\n",
    "# directed random-walk betweenness (via the tnet package)\n",
    "# on Google Colab's R runtime.\n",
    "\n",
    "r_script = \"\"\"\\\n",
    "# compute_rwbc.R\n",
    "# ----------------------------------------------\n",
    "# Install tnet if not already available\n",
    "if (!requireNamespace(\"tnet\", quietly=TRUE)) {\n",
    "  install.packages(\"tnet\", repos=\"https://cloud.r-project.org\")\n",
    "}\n",
    "library(tnet)\n",
    "\n",
    "# Read your edge list; adjust filename if different\n",
    "# Expected format: supplier<TAB>buyer<TAB>weight\n",
    "edges <- read.csv(\"supply.tsv\", sep=\"\\\\t\", header=FALSE,\n",
    "                  col.names=c(\"i\",\"j\",\"w\"))\n",
    "\n",
    "# Convert to a weighted one‐mode tnet graph\n",
    "g <- as.tnet(edges, type=\"weighted one-mode tnet\")\n",
    "\n",
    "# Compute exact random-walk betweenness centrality\n",
    "# (rw_betweenness_w returns a two-column matrix: node, centrality)\n",
    "rwbc_mat <- rw_betweenness_w(g)\n",
    "\n",
    "# Turn into a data frame and save\n",
    "df <- data.frame(\n",
    "  node = rwbc_mat[,1],\n",
    "  rwbc = rwbc_mat[,2]\n",
    ")\n",
    "write.csv(df, \"rwbc_results.csv\", row.names=FALSE)\n",
    "\n",
    "cat(\"Done! Results are in rwbc_results.csv\\n\")\n",
    "\"\"\"\n",
    "\n",
    "# Write the R script to disk\n",
    "with open(\"compute_rwbc.R\", \"w\") as f:\n",
    "    f.write(r_script)\n",
    "\n",
    "print(\"Wrote compute_rwbc.R — upload this (and your supply.tsv) to Colab!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sparsified adjacency matrix from dfz_s_2010.parquet ...\n",
      "Loaded DataFrame of shape (2063, 2063)\n",
      "Directed graph has 396891 edges and 2063 nodes.\n",
      "\n",
      "Calculating normal (shortest-path) betweenness centrality for the directed graph ...\n",
      "Normal betweenness centrality computed in 175.80 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import time\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Read the sparsified graph\n",
    "# -------------------------------\n",
    "print(\"Loading sparsified adjacency matrix from dfz_s_2010.parquet ...\")\n",
    "df = pd.read_parquet('dfz_s_2010.parquet')\n",
    "df.index = df.columns\n",
    "print(f\"Loaded DataFrame of shape {df.shape}\")\n",
    "\n",
    "# Create a directed graph from the DataFrame:\n",
    "G_directed = nx.from_pandas_adjacency(df, create_using=nx.DiGraph())\n",
    "print(f\"Directed graph has {G_directed.number_of_edges()} edges and {G_directed.number_of_nodes()} nodes.\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Compute Normal Betweenness Centrality (Directed)\n",
    "# -------------------------------\n",
    "print(\"Calculating normal (shortest-path) betweenness centrality for the directed graph ...\")\n",
    "start_time = time.time()\n",
    "# This function supports directed graphs.\n",
    "bc_directed = nx.betweenness_centrality(G_directed,\n",
    "                                        normalized=True,\n",
    "                                        weight='weight',\n",
    "                                        endpoints=False)\n",
    "end_time = time.time()\n",
    "time_normal = end_time - start_time\n",
    "print(f\"Normal betweenness centrality computed in {time_normal:.2f} seconds.\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of Normal Betweenness Centrality (directed):\n",
      "  Node AUS_AGRagr: 0.00000\n",
      "  Node AUS_AGRfor: 0.06559\n",
      "  Node AUS_AGRfis: 0.00341\n",
      "  Node AUS_ENRcoa: 0.00000\n",
      "  Node AUS_ENRoil: 0.00000\n",
      "  Node AUS_ENRgas: 0.00000\n",
      "  Node AUS_MIN+: 0.00000\n",
      "  Node AUS_MANfoo: 0.00145\n",
      "  Node AUS_MANtex: 0.00048\n",
      "  Node AUS_MANwoo: 0.00145\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample of Normal Betweenness Centrality (directed):\")\n",
    "for i, (node, value) in enumerate(bc_directed.items()):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(f\"  Node {node}: {value:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 nodes with the highest betweenness centrality:\n",
      "1. Node: MLT_WATwat, Betweenness Centrality: 0.68652\n",
      "2. Node: MLT_AGRfor, Betweenness Centrality: 0.59062\n",
      "3. Node: MLT_MANrep, Betweenness Centrality: 0.30407\n",
      "4. Node: AUS_TRApos, Betweenness Centrality: 0.28972\n",
      "5. Node: IDN_AGRfor, Betweenness Centrality: 0.28246\n",
      "6. Node: SVN_AGRfor, Betweenness Centrality: 0.25463\n",
      "7. Node: LTU_MANfoo, Betweenness Centrality: 0.24856\n",
      "8. Node: LUX_AGRfis, Betweenness Centrality: 0.20947\n",
      "9. Node: RoW_MANcom, Betweenness Centrality: 0.19318\n",
      "10. Node: CHE_WATwat, Betweenness Centrality: 0.18628\n",
      "11. Node: CHE_AGRfis, Betweenness Centrality: 0.16272\n",
      "12. Node: NOR_AGRfor, Betweenness Centrality: 0.15103\n",
      "13. Node: BGR_AGRagr, Betweenness Centrality: 0.14948\n",
      "14. Node: IRL_WATwat, Betweenness Centrality: 0.14891\n",
      "15. Node: SVN_AGRagr, Betweenness Centrality: 0.14449\n",
      "16. Node: JPN_AGRfor, Betweenness Centrality: 0.13726\n",
      "17. Node: MLT_MANwoo, Betweenness Centrality: 0.12934\n",
      "18. Node: IRL_MANtex, Betweenness Centrality: 0.11845\n",
      "19. Node: LUX_ENRoil, Betweenness Centrality: 0.11708\n",
      "20. Node: RoW_ENRcoa, Betweenness Centrality: 0.10383\n",
      "21. Node: PRT_AGRfis, Betweenness Centrality: 0.09844\n",
      "22. Node: GRC_AGRagr, Betweenness Centrality: 0.09178\n",
      "23. Node: FIN_AGRfor, Betweenness Centrality: 0.08797\n",
      "24. Node: SVK_AGRfor, Betweenness Centrality: 0.07990\n",
      "25. Node: AUS_TRAwat, Betweenness Centrality: 0.07309\n",
      "26. Node: RUS_AGRfis, Betweenness Centrality: 0.07306\n",
      "27. Node: RUS_AGRfor, Betweenness Centrality: 0.07126\n",
      "28. Node: CZE_MANpri, Betweenness Centrality: 0.07013\n",
      "29. Node: ROU_ENRoil, Betweenness Centrality: 0.06654\n",
      "30. Node: EST_ENRcoa, Betweenness Centrality: 0.06563\n",
      "31. Node: AUS_AGRfor, Betweenness Centrality: 0.06559\n",
      "32. Node: AUT_WATwat, Betweenness Centrality: 0.06412\n",
      "33. Node: SVK_ENRcoa, Betweenness Centrality: 0.05513\n",
      "34. Node: MLT_MANpri, Betweenness Centrality: 0.04935\n",
      "35. Node: USA_MANpap, Betweenness Centrality: 0.04801\n",
      "36. Node: CYP_ENRcoa, Betweenness Centrality: 0.04678\n",
      "37. Node: RoW_MANmet, Betweenness Centrality: 0.04502\n",
      "38. Node: LUX_ENRcoa, Betweenness Centrality: 0.04502\n",
      "39. Node: CYP_FD+, Betweenness Centrality: 0.04454\n",
      "40. Node: USA_AGRfor, Betweenness Centrality: 0.04191\n",
      "41. Node: MLT_MANpap, Betweenness Centrality: 0.04175\n",
      "42. Node: USA_FINser, Betweenness Centrality: 0.04051\n",
      "43. Node: GRC_MANmet, Betweenness Centrality: 0.03699\n",
      "44. Node: AUT_ENRcoa, Betweenness Centrality: 0.03494\n",
      "45. Node: GBR_WATwat, Betweenness Centrality: 0.03445\n",
      "46. Node: MLT_MANmet, Betweenness Centrality: 0.03422\n",
      "47. Node: SVN_AGRfis, Betweenness Centrality: 0.02995\n",
      "48. Node: IND_MANrep, Betweenness Centrality: 0.02950\n",
      "49. Node: LVA_MANpap, Betweenness Centrality: 0.02869\n",
      "50. Node: ESP_AGRfis, Betweenness Centrality: 0.02784\n",
      "51. Node: SVK_AGRfis, Betweenness Centrality: 0.02401\n",
      "52. Node: NLD_AGRfor, Betweenness Centrality: 0.02359\n",
      "53. Node: AUS_WATwst, Betweenness Centrality: 0.02240\n",
      "54. Node: HRV_AGRfor, Betweenness Centrality: 0.02186\n",
      "55. Node: BEL_ENRcoa, Betweenness Centrality: 0.02183\n",
      "56. Node: DEU_WATwat, Betweenness Centrality: 0.02132\n",
      "57. Node: LUX_AGRagr, Betweenness Centrality: 0.02104\n",
      "58. Node: BRA_AGRfis, Betweenness Centrality: 0.02102\n",
      "59. Node: CYP_TRApos, Betweenness Centrality: 0.02099\n",
      "60. Node: CYP_MIN+, Betweenness Centrality: 0.02087\n",
      "61. Node: CYP_AGRfor, Betweenness Centrality: 0.02086\n",
      "62. Node: RoW_ENRgas, Betweenness Centrality: 0.01955\n",
      "63. Node: LTU_ENRcoa, Betweenness Centrality: 0.01943\n",
      "64. Node: ROU_AGRfis, Betweenness Centrality: 0.01940\n",
      "65. Node: DNK_AGRfis, Betweenness Centrality: 0.01907\n",
      "66. Node: HRV_AGRfis, Betweenness Centrality: 0.01824\n",
      "67. Node: GBR_AGRfis, Betweenness Centrality: 0.01812\n",
      "68. Node: CZE_AGRfis, Betweenness Centrality: 0.01718\n",
      "69. Node: EST_AGRfor, Betweenness Centrality: 0.01597\n",
      "70. Node: NLD_AGRagr, Betweenness Centrality: 0.01591\n",
      "71. Node: LVA_ENRgas, Betweenness Centrality: 0.01590\n",
      "72. Node: LUX_AGRfor, Betweenness Centrality: 0.01545\n",
      "73. Node: GRC_MANpap, Betweenness Centrality: 0.01527\n",
      "74. Node: BGR_AGRfis, Betweenness Centrality: 0.01525\n",
      "75. Node: FIN_AGRfis, Betweenness Centrality: 0.01456\n",
      "76. Node: POL_AGRfor, Betweenness Centrality: 0.01419\n",
      "77. Node: MLT_ENRcoa, Betweenness Centrality: 0.01402\n",
      "78. Node: NLD_AGRfis, Betweenness Centrality: 0.01376\n",
      "79. Node: HUN_WATwat, Betweenness Centrality: 0.01348\n",
      "80. Node: FIN_WATwat, Betweenness Centrality: 0.01347\n",
      "81. Node: EST_AGRfis, Betweenness Centrality: 0.01323\n",
      "82. Node: LTU_MANpap, Betweenness Centrality: 0.01243\n",
      "83. Node: BGR_AGRfor, Betweenness Centrality: 0.01240\n",
      "84. Node: HUN_AGRfis, Betweenness Centrality: 0.01236\n",
      "85. Node: POL_AGRfis, Betweenness Centrality: 0.01167\n",
      "86. Node: LUX_TRApos, Betweenness Centrality: 0.01114\n",
      "87. Node: KOR_AGRfor, Betweenness Centrality: 0.01052\n",
      "88. Node: HRV_TRApos, Betweenness Centrality: 0.01022\n",
      "89. Node: SVK_AGRagr, Betweenness Centrality: 0.01017\n",
      "90. Node: SVN_TRAwat, Betweenness Centrality: 0.00991\n",
      "91. Node: BGR_MIN+, Betweenness Centrality: 0.00968\n",
      "92. Node: CZE_MANpap, Betweenness Centrality: 0.00951\n",
      "93. Node: LTU_AGRagr, Betweenness Centrality: 0.00941\n",
      "94. Node: CZE_AGRfor, Betweenness Centrality: 0.00939\n",
      "95. Node: DNK_AGRfor, Betweenness Centrality: 0.00935\n",
      "96. Node: CYP_MANpap, Betweenness Centrality: 0.00933\n",
      "97. Node: PRT_AGRfor, Betweenness Centrality: 0.00923\n",
      "98. Node: ROU_MANpap, Betweenness Centrality: 0.00921\n",
      "99. Node: RoW_ENRoil, Betweenness Centrality: 0.00902\n",
      "100. Node: LUX_MANtra, Betweenness Centrality: 0.00872\n"
     ]
    }
   ],
   "source": [
    "# Sort nodes by betweenness centrality in descending order\n",
    "sorted_bc = sorted(bc_directed.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Print the top 100 nodes\n",
    "print(\"Top 100 nodes with the highest betweenness centrality:\")\n",
    "for rank, (node, centrality) in enumerate(sorted_bc[:100], start=1):\n",
    "    print(f\"{rank}. Node: {node}, Betweenness Centrality: {centrality:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Compute Random Walk (Current-Flow) Betweenness Centrality\n",
    "# -------------------------------\n",
    "# NOTE: current_flow_betweenness_centrality does not support DiGraphs.\n",
    "# As a workaround, we convert the directed graph into an undirected one.\n",
    "print(\"Converting the directed graph to undirected for current-flow betweenness centrality computation ...\")\n",
    "G_undirected = G_directed.to_undirected()\n",
    "print(f\"Undirected graph has {G_undirected.number_of_edges()} edges and {G_undirected.number_of_nodes()} nodes.\\n\")\n",
    "\n",
    "print(\"Calculating random walk (current-flow) betweenness centrality (this may be slow ...)\")\n",
    "\n",
    "start_time = time.time()\n",
    "# You can choose the solver; here we use 'lu' for moderate memory usage.\n",
    "rw_bc = nx.current_flow_betweenness_centrality(G_undirected,\n",
    "                                               normalized=True,\n",
    "                                               weight='weight',\n",
    "                                               solver='lu')\n",
    "end_time = time.time()\n",
    "time_rw = end_time - start_time\n",
    "print(f\"Random walk (current-flow) betweenness centrality computed in {time_rw:.2f} seconds.\\n\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Display a Sample of the Results\n",
    "# -------------------------------\n",
    "print(\"\\nSample of Random Walk (Current-Flow) Betweenness Centrality (computed on undirected graph):\")\n",
    "for i, (node, value) in enumerate(rw_bc.items()):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(f\"  Node {node}: {value:.5f}\")\n",
    "\n",
    "print(\"\\nSummary of Computation Times:\")\n",
    "print(f\"  Normal Betweenness Centrality: {time_normal:.2f} seconds\")\n",
    "print(f\"  Random Walk Betweenness Centrality: {time_rw:.2f} seconds\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Decision Point\n",
    "# -------------------------------\n",
    "print(\"\\nBased on the computation times you can decide whether the normal (shortest-path) betweenness centrality is enough or whether the random walk version might provide additional insight at acceptable performance cost.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
