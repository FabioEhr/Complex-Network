{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6669cc70-91f6-459c-8793-8c7da26db55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from  matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1c3b95f-339d-4cda-8534-b647eb2e9110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def a function to create cumulative dict for countries\n",
    "def cum_count(D):\n",
    "    cum_count_D = {}\n",
    "    # Loop over each key in the original dictionary\n",
    "    for key, value in D.items():\n",
    "        # Extract the prefix (the part before the underscore)\n",
    "        prefix = key.split('_')[0]\n",
    "    \n",
    "        # Sum the values for each prefix\n",
    "        if prefix in cum_count_D:\n",
    "            cum_count_D[prefix] += value\n",
    "        else:\n",
    "            cum_count_D[prefix] = value\n",
    "    return cum_count_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3154d829-2fca-48a4-a3a4-af7499a0c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2010, 2021)\n",
    "labels = ['eu', 'gl', 'bc']\n",
    "all_identifiers = list(years) + labels \n",
    "for identifier in all_identifiers:\n",
    "    globals()[f\"dfz_{identifier}\"]  = pd.read_parquet(f'dfz_{identifier}.parquet')\n",
    "    # Create the directed, weighted graph from DataFrame\n",
    "    globals()[f\"G_{identifier}\"]= nx.from_pandas_adjacency(globals()[f\"dfz_{identifier}\"], create_using=nx.DiGraph)\n",
    "\n",
    "    # Degrees\n",
    "    in_degrees = dict( globals()[f\"G_{identifier}\"].in_degree())\n",
    "    out_degrees = dict( globals()[f\"G_{identifier}\"].out_degree())\n",
    "    in_degree_values = list(in_degrees.values())\n",
    "    out_degree_values = list(out_degrees.values())\n",
    "\n",
    "    std_in_degree = np.std(in_degree_values)\n",
    "    std_out_degree = np.std(out_degree_values)\n",
    "\n",
    "    # Check completeness\n",
    "    is_complete = std_in_degree == 0 and std_out_degree == 0\n",
    "    title_year = f\"{identifier} (complete)\" if is_complete else f\"{identifier}\"\n",
    "\n",
    "    # Extract weights\n",
    "    weights =  globals()[f\"dfz_{identifier}\"].values.flatten()\n",
    "    mean_weight = np.mean(weights)\n",
    "    std_weight = np.std(weights)\n",
    "\n",
    "    # In-strength and Out-strength\n",
    "    in_str = {node: sum(data['weight'] for _, _, data in globals()[f\"G_{identifier}\"].in_edges(node, data=True)) for node in globals()[f\"G_{identifier}\"].nodes}\n",
    "    out_str = {node: sum(data['weight'] for _, _, data in globals()[f\"G_{identifier}\"].out_edges(node, data=True)) for node in globals()[f\"G_{identifier}\"].nodes}\n",
    "    in_str_val = list(in_str.values())\n",
    "    out_str_val = list(out_str.values())\n",
    "    \n",
    "    mean_in_str = np.mean(in_str_val)\n",
    "    std_in_str = np.std(in_str_val)\n",
    "    \n",
    "    mean_out_str = np.mean(out_str_val)\n",
    "    std_out_str = np.std(out_str_val)\n",
    "    \n",
    "    # Hubs and Authorities\n",
    "    hub_aut = nx.hits(globals()[f\"G_{identifier}\"])\n",
    "    hub_c = cum_count(hub_aut[0])\n",
    "    aut_c = cum_count(hub_aut[1])\n",
    "    keys = list(hub_c.keys())\n",
    "    aut_val = [aut_c[key] for key in keys]\n",
    "    hub_val = [hub_c[key] for key in keys]\n",
    "    \n",
    "    # --- Plotting ---\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(title_year, fontsize=18, x=0.01, y=0.98, ha='left')\n",
    "    \n",
    "    # Box style for stats\n",
    "    box_style = dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.4', alpha=0.9)\n",
    "    \n",
    "    # 1. Edge Weight Distribution\n",
    "    axs[0, 0].hist(weights, bins=np.logspace(np.log10(min(weights)), np.log10(max(weights)), 100), color='g', alpha=0.7)\n",
    "    axs[0, 0].set_xscale('log')\n",
    "    axs[0, 0].set_title('Edge Weight Distribution')\n",
    "    axs[0, 0].set_xlabel('Edge Weight (k$)')\n",
    "    axs[0, 0].set_ylabel('Frequency')\n",
    "    axs[0, 0].text(\n",
    "        0.95, 0.95,\n",
    "        f'Mean: {mean_weight:.2f}\\nStd: {std_weight:.2f}',\n",
    "        transform=axs[0, 0].transAxes,\n",
    "        fontsize=10,\n",
    "        verticalalignment='top',\n",
    "        horizontalalignment='right',\n",
    "        bbox=box_style\n",
    "    )\n",
    "    \n",
    "    # 2. In-strength Distribution\n",
    "    axs[0, 1].hist(in_str_val, bins=50, color='b', alpha=0.7)\n",
    "    axs[0, 1].set_title('In-strength Distribution')\n",
    "    axs[0, 1].set_xlabel('In-strength (k$)')\n",
    "    axs[0, 1].set_ylabel('Frequency')\n",
    "    axs[0, 1].text(\n",
    "        0.95, 0.95,\n",
    "        f'Mean: {mean_in_str:.2f}\\nStd: {std_in_str:.2f}',\n",
    "        transform=axs[0, 1].transAxes,\n",
    "        fontsize=10,\n",
    "        verticalalignment='top',\n",
    "        horizontalalignment='right',\n",
    "        bbox=box_style\n",
    "    )\n",
    "    \n",
    "    # 3. Out-strength Distribution\n",
    "    axs[1, 0].hist(out_str_val, bins=50, color='r', alpha=0.7)\n",
    "    axs[1, 0].set_title('Out-strength Distribution')\n",
    "    axs[1, 0].set_xlabel('Out-strength (K$)')\n",
    "    axs[1, 0].set_ylabel('Frequency')\n",
    "    axs[1, 0].text(\n",
    "        0.95, 0.95,\n",
    "        f'Mean: {mean_out_str:.2f}\\nStd: {std_out_str:.2f}',\n",
    "        transform=axs[1, 0].transAxes,\n",
    "        fontsize=10,\n",
    "        verticalalignment='top',\n",
    "        horizontalalignment='right',\n",
    "        bbox=box_style\n",
    "    )\n",
    "    \n",
    "    # 4. Hubs vs Authorities\n",
    "    axs[1, 1].scatter(aut_val, hub_val)\n",
    "    for i, key in enumerate(keys):\n",
    "        axs[1, 1].annotate(key, (aut_val[i], hub_val[i]), fontsize=8)\n",
    "    axs[1, 1].set_title('Hubs vs Authorities')\n",
    "    axs[1, 1].set_xlabel('Authority')\n",
    "    axs[1, 1].set_ylabel('Hubness')\n",
    "    axs[1, 1].grid(True)\n",
    "    \n",
    "    # Layout and save\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.savefig(f'Base_measures_{identifier}.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Save hub and authorities\n",
    "    hub_scores = hub_aut[0]\n",
    "    auth_scores = hub_aut[1]\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df_hub_auth = pd.DataFrame({\n",
    "        'node': list(hub_scores.keys()),\n",
    "        'hub_score': list(hub_scores.values()),\n",
    "        'authority_score': [auth_scores[k] for k in hub_scores.keys()]\n",
    "    })\n",
    "    \n",
    "    # Save to Parquet\n",
    "    df_hub_auth.to_parquet(f'hub_aut_{identifier}.parquet', index=False)\n",
    "    print(f\"Saved: hub_aut_{identifier}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073c6fe-fd79-472d-acff-185ab65fed70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
