{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b9726d3-999f-443a-8cfe-c2e0480fcfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from  matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53dd43ce-1c10-4a4b-b31b-6cfa106cb894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_edge_weights(G):\n",
    "    G_copy = G.copy()\n",
    "    # Extract all edge weights\n",
    "    weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "    \n",
    "    w_min, w_max = min(weights), max(weights)\n",
    "    \n",
    "    # Avoid division by zero if all weights are the same\n",
    "    if w_max == w_min:\n",
    "        return G_copy  # No need to normalize if all weights are identical\n",
    "    \n",
    "    # Normalize each edge weight in the copied graph\n",
    "    for u, v in G_copy.edges():\n",
    "        G_copy[u][v]['weight'] = (G_copy[u][v]['weight'] - w_min) / (w_max - w_min)\n",
    "    \n",
    "    return G_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb3c9d1d-7842-4666-8c80-ba628a3d57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_by_sector_inflow(G, threshold, weight_key='weight'):\n",
    "    \"\"\"\n",
    "    Create a new DiGraph H where, for each buyer node v,\n",
    "    we group its in-edges by the supplier sector and keep\n",
    "    only those edges >= threshold * (total inflow from that sector).\n",
    "\n",
    "    Then, for each node, if the sum of its outgoing edges in H is\n",
    "    less than 50% of the sum of its outgoing edges in G, we add\n",
    "    the highest-weight outgoing edges from G until we reach >= 50%.\n",
    "\n",
    "    Assumes each node name looks like 'COUNTRY_SECTOR',\n",
    "    e.g. 'FRA_MANche' or 'CHN_MANche'.\n",
    "    \"\"\"\n",
    "    H = nx.DiGraph()\n",
    "\n",
    "    # --- 1) Keep edges >= threshold * sector inflow ---\n",
    "    for v in G.nodes():\n",
    "        # All in-edges to v\n",
    "        in_edges_data = G.in_edges(v, data=True)\n",
    "\n",
    "        # 1A) Sum flows by supplier sector\n",
    "        sector_in_sum = {}\n",
    "        for (u, _, data) in in_edges_data:\n",
    "            w = data.get(weight_key, 0.0)\n",
    "            try:\n",
    "                # Split \"FRA_MANche\" into \"FRA\" and \"MANche\"\n",
    "                supplier_country, supplier_sector = u.split('_', 1)\n",
    "            except ValueError:\n",
    "                # If node naming doesn't match, skip or handle differently\n",
    "                continue\n",
    "            sector_in_sum.setdefault(supplier_sector, 0.0)\n",
    "            sector_in_sum[supplier_sector] += w\n",
    "\n",
    "        # 1B) Keep only edges >= threshold * total_for_that_sector\n",
    "        for (u, _, data) in in_edges_data:\n",
    "            w = data.get(weight_key, 0.0)\n",
    "            try:\n",
    "                supplier_country, supplier_sector = u.split('_', 1)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            total_for_sector = sector_in_sum.get(supplier_sector, 0.0)\n",
    "            if total_for_sector > 0 and w >= threshold * total_for_sector:\n",
    "                # Add edge to H\n",
    "                H.add_edge(u, v, **{weight_key: w})\n",
    "\n",
    "    # --- 2) Ensure each node's outgoing edges in H reach >= 50% of G's total ---\n",
    "    for node in H.nodes():\n",
    "        # 2A) Sum of outgoing edges in H\n",
    "        out_edges_H = H.out_edges(node, data=True)\n",
    "        sum_H = sum(data.get(weight_key, 0.0) for _, _, data in out_edges_H)\n",
    "\n",
    "        # 2B) Sum of outgoing edges in G\n",
    "        out_edges_G = G.out_edges(node, data=True)\n",
    "        sum_G = sum(data.get(weight_key, 0.0) for _, _, data in out_edges_G)\n",
    "\n",
    "        # 2C) If < 50%, add highest-weight edges from G until we reach >= 50%\n",
    "        if sum_G > 0:\n",
    "            ratio = sum_H / sum_G\n",
    "            if ratio < 0.5:\n",
    "                # Sort G's outgoing edges (descending by weight)\n",
    "                sorted_out_edges = sorted(\n",
    "                    out_edges_G,\n",
    "                    key=lambda x: x[2].get(weight_key, 0.0),\n",
    "                    reverse=True\n",
    "                )\n",
    "                # Add edges from G until ratio >= 0.5 (or no more edges)\n",
    "                for u, v, data in sorted_out_edges:\n",
    "                    w_g = data.get(weight_key, 0.0)\n",
    "                    if w_g <= 0:\n",
    "                        continue  # Skip zero-weight edges\n",
    "\n",
    "                    # If H doesn't have this edge or has a smaller weight, update it\n",
    "                    if H.has_edge(u, v):\n",
    "                        current_w = H[u][v].get(weight_key, 0.0)\n",
    "                    else:\n",
    "                        current_w = 0.0\n",
    "\n",
    "                    if current_w < w_g:\n",
    "                        # Add/update in H\n",
    "                        H.add_edge(u, v, **{weight_key: w_g})\n",
    "                        # Update sum_H and check ratio\n",
    "                        sum_H = sum_H - current_w + w_g\n",
    "                        ratio = sum_H / sum_G\n",
    "                        if ratio >= 0.5:\n",
    "                            break\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2d7e27-12fb-424f-888f-bfd469e4e6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original graph edges: 4255969\n",
      "Sparsified graph edges: 343382\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# EXAMPLE USAGE\n",
    "# -----------------------------\n",
    "# Apply the sector-based 2% inflow sparsification to the original Graph G (data from 2020)\n",
    "S_20 = sparsify_by_sector_inflow(G, threshold = 0.02,  weight_key='weight')\n",
    "\n",
    "#Convert the new graph H back to a DataFrame if you wish\n",
    "dfz_sparsified = nx.to_pandas_adjacency(S_20, dtype=float)\n",
    "#dfz_sparsified.to_parquet('dfz_sparsified.parquet')\n",
    "\n",
    "# Check how many edges remain\n",
    "print(f\"Original graph edges: {G.number_of_edges()}\")\n",
    "print(f\"Sparsified graph edges: {S_20.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edd997f4-4353-4176-9191-e8e871378c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the years and custom labels\n",
    "years = range(2017, 2021)\n",
    "labels = ['eu', 'gl', 'bc']\n",
    "all_identifiers = list(years) + labels\n",
    "\n",
    "# Loop through all identifiers\n",
    "for identifier in all_identifiers:\n",
    "    globals()[f\"dfz_{identifier}\"]  = pd.read_parquet(f'dfz_{identifier}.parquet')\n",
    "    # Create the directed, weighted graph from DataFrame\n",
    "    globals()[f\"G_{identifier}\"]= nx.from_pandas_adjacency(globals()[f\"dfz_{identifier}\"], create_using=nx.DiGraph)\n",
    "\n",
    "for identifier in years:\n",
    "    globals()[f\"S_{identifier}\"] = sparsify_by_sector_inflow( globals()[f\"G_{identifier}\"], threshold = 0.023,  weight_key='weight')\n",
    "    globals()[f\"dfz_s_{identifier}\"] = nx.to_pandas_adjacency(globals()[f\"S_{identifier}\"], dtype=float)\n",
    "    globals()[f\"dfz_s_{identifier}\"].to_parquet(f'dfz_s_{identifier}.parquet', engine=\"pyarrow\", index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcf6710a-c964-47f8-9c18-5438a1507af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: clus_2017.parquet\n",
      "Saved: clus_2018.parquet\n",
      "Saved: clus_2019.parquet\n",
      "Saved: clus_2020.parquet\n",
      "Saved: clus_eu.parquet\n",
      "Saved: clus_gl.parquet\n",
      "Saved: clus_bc.parquet\n"
     ]
    }
   ],
   "source": [
    "for identifier in all_identifiers:\n",
    "    clus_coeff = nx.clustering(globals()[f\"S_{identifier}\"], weight='weight')\n",
    "    df = pd.DataFrame({\"Node\": list(clus_coeff.keys()), \n",
    "                       \"Clustering_Coefficient\": list(clus_coeff.values())})\n",
    "    df.to_parquet(f'clus_{identifier}.parquet', engine=\"pyarrow\", index=False)\n",
    "    print(f\"Saved: clus_{identifier}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eac1e557-c4bb-4014-b1d6-ab8cc3f605c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original graph edges: 4255969\n",
      "Sparsified graph edges: 457589\n"
     ]
    }
   ],
   "source": [
    "# Check how many edges remain\n",
    "print(f\"Original graph edges: {G_2017.number_of_edges()}\")\n",
    "print(f\"Sparsified graph edges: {S_2017.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f39465b-dac2-47f7-a719-8cc5b469b7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(2010, 2017)\n",
    "# Loop through all identifiers\n",
    "for identifier in years:\n",
    "    globals()[f\"dfz_{identifier}\"]  = pd.read_parquet(f'dfz_{identifier}.parquet')\n",
    "    # Create the directed, weighted graph for bca scenario from DataFrame\n",
    "    globals()[f\"G_{identifier}\"]= nx.from_pandas_adjacency(globals()[f\"dfz_{identifier}\"], create_using=nx.DiGraph)\n",
    "   \n",
    "\n",
    "for identifier in years:\n",
    "    globals()[f\"S_{identifier}\"] = sparsify_by_sector_inflow( globals()[f\"G_{identifier}\"], threshold = 0.023,  weight_key='weight')\n",
    "    globals()[f\"dfz_s_{identifier}\"] = nx.to_pandas_adjacency(globals()[f\"S_{identifier}\"], dtype=float)\n",
    "    globals()[f\"dfz_s_{identifier}\"].to_parquet(f'dfz_s_{identifier}.parquet', engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3fec0d3-fb18-4c08-b98c-829307f8af7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: clus_2010.parquet\n",
      "Saved: clus_2011.parquet\n",
      "Saved: clus_2012.parquet\n",
      "Saved: clus_2013.parquet\n",
      "Saved: clus_2014.parquet\n",
      "Saved: clus_2015.parquet\n",
      "Saved: clus_2016.parquet\n"
     ]
    }
   ],
   "source": [
    "for identifier in years:\n",
    "    clus_coeff = nx.clustering(globals()[f\"S_{identifier}\"], weight='weight')\n",
    "    df = pd.DataFrame({\"Node\": list(clus_coeff.keys()), \n",
    "                       \"Clustering_Coefficient\": list(clus_coeff.values())})\n",
    "    df.to_parquet(f'clus_{identifier}.parquet', engine=\"pyarrow\", index=False)\n",
    "    print(f\"Saved: clus_{identifier}.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
